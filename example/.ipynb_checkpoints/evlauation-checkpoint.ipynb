{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def parse_data(file_name):\n",
    "    features = list()\n",
    "    labels = list()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            if bool(re.search(\"^[0-9]\", l)):\n",
    "                g = re.search(\"^(([0-9]{1,2},?)+)\\s(.*)$\", l)\n",
    "                labels.append([int(i) for i in g.group(1).split(\",\")])\n",
    "                features.append(eval(\"{\" + re.sub(\"\\s\", \",\", g.group(3)) + \"}\"))\n",
    "            else:\n",
    "                l = l.strip()\n",
    "                labels.append([])\n",
    "                features.append(eval(\"{\" + re.sub(\"\\s\", \",\", l) + \"}\"))\n",
    "    features = pd.DataFrame.from_dict(features).fillna(0).iloc[:,:].values\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(labels)\n",
    "    return features, y\n",
    "\n",
    "features, y = parse_data(\"data.txt\")\n",
    "print(features.shape)\n",
    "print(y.shape)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# the 'explorer' polcy will be fit with this small sample of the rows\n",
    "st_seed = 0\n",
    "end_seed = 3000\n",
    "\n",
    "# then it will choose actions for this larger sample\n",
    "st_exploration = 0\n",
    "end_exploration = 5000\n",
    "\n",
    "# the new policy will be evaluated with a separate test set\n",
    "st_test = 5000\n",
    "end_test = 7395\n",
    "\n",
    "# separating the covariates data for each case\n",
    "Xseed = features[st_seed:end_seed, :]\n",
    "Xexplore_sample = features[st_exploration:end_exploration, :]\n",
    "Xtest = features[st_test:end_test, :]\n",
    "nchoices = y.shape[1]\n",
    "\n",
    "# now constructing an exploration policy as explained above, with fully-labeled data\n",
    "explorer = LogisticRegression()\n",
    "explorer.fit(Xseed, np.argmax(y[st_seed:end_seed], axis=1))\n",
    "\n",
    "# letting the exploration policy choose actions for the new policy input\n",
    "actions_explore_sample = explorer.predict(Xexplore_sample)\n",
    "rewards_explore_sample = y[st_exploration:end_exploration, :]\\\n",
    "                        [np.arange(end_exploration - st_exploration), actions_explore_sample]\n",
    "\n",
    "# extracting the probabilities it estimated\n",
    "ix_internal_actions = {j:i for i,j in enumerate(explorer.classes_)}\n",
    "ix_internal_actions = [ix_internal_actions[i] for i in actions_explore_sample]\n",
    "ix_internal_actions = np.array(ix_internal_actions)\n",
    "prob_actions_explore = explorer.predict_proba(Xexplore_sample)[np.arange(Xexplore_sample.shape[0]),\n",
    "                                                               ix_internal_actions]\n",
    "\n",
    "from contextualbandits.online import SeparateClassifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(), nchoices=y.shape[1],\n",
    "                                 beta_prior=None, smoothing=None)\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample)\n",
    "mean_reward_naive = np.mean(y[st_test:end_test, :]\\\n",
    "                             [np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Separate Classifiers: \", mean_reward_naive)\n",
    "\n",
    "from contextualbandits.online import SeparateClassifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(), nchoices=y.shape[1],\n",
    "                                 beta_prior=\"auto\")\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample)\n",
    "mean_reward_beta = np.mean(y[st_test:end_test, :]\\\n",
    "                            [np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Separate Classifiers + Prior: \", mean_reward_beta)\n",
    "\n",
    "from contextualbandits.online import SeparateClassifiers\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = SeparateClassifiers(base_algorithm=LogisticRegression(), nchoices=y.shape[1],\n",
    "                                 beta_prior=None, smoothing = (1,2))\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample)\n",
    "mean_reward_sm = np.mean(y[st_test:end_test, :]\\\n",
    "                            [np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Separate Classifiers + Smoothing: \", mean_reward_sm)\n",
    "\n",
    "from contextualbandits.offpolicy import OffsetTree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "new_policy = OffsetTree(base_algorithm=LogisticRegression(), nchoices=y.shape[1])\n",
    "new_policy.fit(X=Xexplore_sample, a=actions_explore_sample, r=rewards_explore_sample, p=prob_actions_explore)\n",
    "mean_reward_ot = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "print(\"Test set mean reward - Offset Tree technique: \", mean_reward_ot)\n",
    "\n",
    "from contextualbandits.offpolicy import DoublyRobustEstimator\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "\n",
    "import matplotlib.pyplot as plt, pandas as pd\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "%matplotlib inline\n",
    "\n",
    "results = pd.DataFrame({\n",
    "    'Off-policy Learning Method' : ['Naive', 'Naive + Prior', 'Naive + Smoothing', 'Offset Tree'],\n",
    "    'Test set mean reward' : [mean_reward_naive, mean_reward_beta, mean_reward_sm, mean_reward_ot]\n",
    "})\n",
    "\n",
    "sns.set(font_scale = 1.3)\n",
    "rcParams['figure.figsize'] = 22, 7\n",
    "sns.barplot(x = \"Off-policy Learning Method\", y=\"Test set mean reward\", data=results)\n",
    "plt.title('Off-policy Learning on MPTCP Dataset\\nBase Classifier is Logistic Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33681, 38)\n",
      "(33681, 2)\n",
      "Test set Rejection Sampling mean reward estimate (new policy)\n",
      "Estimated mean reward:  0.9606471359860078\n",
      "Sample size:  6861\n",
      "----------------\n",
      "Real mean reward:  0.9611870477304291\n",
      "Test set Rejection Sampling mean reward estimate (old policy)\n",
      "Estimated mean reward:  0.9606471359860078\n",
      "Sample size:  6861\n",
      "----------------\n",
      "Real mean reward:  0.9611870477304291\n",
      "Biased Test set Rejection Sampling mean reward estimate (new policy)\n",
      "Estimated mean reward:  0.9611870477304291\n",
      "Sample size:  13681\n",
      "----------------\n",
      "Real mean reward:  0.9611870477304291\n",
      "(Don't try rejection sampling on a biased test set)\n",
      "Biased Test set mean reward estimates (new policy)\n",
      "DR estimate (reward estimator fit on train+test):  1.036219380156284\n",
      "DR estimate (reward estimator fit on test only):  0.9549009596903066\n",
      "----------------\n",
      "Real mean reward:  0.9611870477304291\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np, re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "def parse_data(file_name):\n",
    "    features = list()\n",
    "    labels = list()\n",
    "    with open(file_name, 'rt') as f:\n",
    "        f.readline()\n",
    "        for l in f:\n",
    "            if bool(re.search(\"^[0-9]\", l)):\n",
    "                g = re.search(\"^(([0-9]{1,2},?)+)\\s(.*)$\", l)\n",
    "                labels.append([int(i) for i in g.group(1).split(\",\")])\n",
    "                features.append(eval(\"{\" + re.sub(\"\\s\", \",\", g.group(3)) + \"}\"))\n",
    "            else:\n",
    "                l = l.strip()\n",
    "                labels.append([])\n",
    "                features.append(eval(\"{\" + re.sub(\"\\s\", \",\", l) + \"}\"))\n",
    "    features = pd.DataFrame.from_dict(features).fillna(0).iloc[:,:].values\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    y = mlb.fit_transform(labels)\n",
    "    return features, y\n",
    "\n",
    "features, y = parse_data(\"data.txt\")\n",
    "print(features.shape)\n",
    "print(y.shape)\n",
    "\n",
    "#Simulating a stationary exploration policy and a test set:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# the 'explorer' polcy will be fit with this small sample of the rows\n",
    "st_seed = 0\n",
    "end_seed = 10000\n",
    "\n",
    "# then it will choose actions for this larger sample, which will be the input for the new policy\n",
    "st_exploration = 0\n",
    "end_exploration = 20000\n",
    "\n",
    "# the new policy will be evaluated with a separate test set\n",
    "st_test = 20000\n",
    "end_test = 33681\n",
    "\n",
    "# separating the covariates data for each case\n",
    "Xseed = features[st_seed:end_seed, :]\n",
    "Xexplore_sample = features[st_exploration:end_exploration, :]\n",
    "Xtest = features[st_test:end_test, :]\n",
    "nchoices = y.shape[1]\n",
    "\n",
    "# now constructing an exploration policy as explained above, with fully-labeled data\n",
    "explorer = LogisticRegression()\n",
    "np.random.seed(100)\n",
    "explorer.fit(Xseed, np.argmax(y[st_seed:end_seed], axis=1))\n",
    "\n",
    "# letting the exploration policy choose actions for the new policy input\n",
    "np.random.seed(100)\n",
    "actions_explore_sample=explorer.predict(Xexplore_sample)\n",
    "rewards_explore_sample=y[st_exploration:end_exploration, :]\\\n",
    "                        [np.arange(end_exploration - st_exploration), actions_explore_sample]\n",
    "\n",
    "# extracting the probabilities it estimated\n",
    "ix_internal_actions = {j:i for i,j in enumerate(explorer.classes_)}\n",
    "ix_internal_actions = [ix_internal_actions[i] for i in actions_explore_sample]\n",
    "ix_internal_actions = np.array(ix_internal_actions)\n",
    "prob_actions_explore = explorer.predict_proba(Xexplore_sample)[np.arange(Xexplore_sample.shape[0]),\n",
    "                                                               ix_internal_actions]\n",
    "\n",
    "# generating a test set with random actions\n",
    "actions_test = np.random.randint(nchoices, size=end_test - st_test)\n",
    "rewards_test = y[st_test:end_test, :][np.arange(end_test - st_test), actions_test]\n",
    "\n",
    "#Rejection sampling estimate:\n",
    "from contextualbandits.online import SeparateClassifiers\n",
    "from contextualbandits.evaluation import evaluateRejectionSampling\n",
    "\n",
    "new_policy = SeparateClassifiers(LogisticRegression(C=0.1), y.shape[1])\n",
    "np.random.seed(100)\n",
    "new_policy.fit(Xexplore_sample, actions_explore_sample, rewards_explore_sample)\n",
    "np.random.seed(100)\n",
    "est_r, ncases = evaluateRejectionSampling(new_policy, X=Xtest, a=actions_test, r=rewards_test, online=False)\n",
    "np.random.seed(100)\n",
    "real_r = np.mean(y[st_test:end_test,:][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "\n",
    "print('Test set Rejection Sampling mean reward estimate (new policy)')\n",
    "print('Estimated mean reward: ',est_r)\n",
    "print('Sample size: ', ncases)\n",
    "print('----------------')\n",
    "print('Real mean reward: ', real_r)\n",
    "\n",
    "#We can also evaluate the exploration policy with the same method:\n",
    "np.random.seed(100)\n",
    "est_r, ncases = evaluateRejectionSampling(explorer, X=Xtest, a=actions_test, r=rewards_test, online=False)\n",
    "real_r = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), explorer.predict(Xtest)])\n",
    "\n",
    "print('Test set Rejection Sampling mean reward estimate (old policy)')\n",
    "print('Estimated mean reward: ', est_r)\n",
    "print('Sample size: ', ncases)\n",
    "print('----------------')\n",
    "print('Real mean reward: ', real_r)\n",
    "\n",
    "#To be stressed again, such an evaluation method only works when the data was collected by choosing\n",
    "#actions at random. If we evaluate it with the actions chosen by the exploration policy, the results will\n",
    "#be totally biased as demonstrated here:\n",
    "\n",
    "actions_test_biased = explorer.predict(Xtest)\n",
    "rewards_test_biased = y[st_test:end_test, :][np.arange(end_test - st_test), actions_test_biased]\n",
    "est_r, ncases = evaluateRejectionSampling(new_policy, X=Xtest, a=actions_test_biased,\\\n",
    "                                              r=rewards_test_biased, online=False)\n",
    "real_r = np.mean(y[st_test:end_test, :][np.arange(end_test - st_test), new_policy.predict(Xtest)])\n",
    "\n",
    "print('Biased Test set Rejection Sampling mean reward estimate (new policy)')\n",
    "print('Estimated mean reward: ', est_r)\n",
    "print('Sample size: ', ncases)\n",
    "print('----------------')\n",
    "print('Real mean reward: ', real_r)\n",
    "print(\"(Don't try rejection sampling on a biased test set)\")\n",
    "\n",
    "#We can also try Doubly-Robust estimates, but these work poorly for a dataset like this:\n",
    "from contextualbandits.evaluation import evaluateDoublyRobust\n",
    "\n",
    "# getting estimated probabilities for the biased test sample chosen by the old policy\n",
    "ix_internal_actions = {j:i for i,j in enumerate(explorer.classes_)}\n",
    "ix_internal_actions = [ix_internal_actions[i] for i in actions_test_biased]\n",
    "ix_internal_actions = np.array(ix_internal_actions)\n",
    "prob_actions_test_biased = explorer.predict_proba(Xtest)[np.arange(Xtest.shape[0]), ix_internal_actions]\n",
    "\n",
    "\n",
    "# actions that the new policy will choose\n",
    "np.random.seed(1)\n",
    "pred = new_policy.predict(Xtest)\n",
    "\n",
    "# method 1: estimating rewards by fitting another model to the whole data (train + test)\n",
    "model_fit_on_all_data = SeparateClassifiers(LogisticRegression(), y.shape[1])\n",
    "np.random.seed(1)\n",
    "model_fit_on_all_data.fit(np.r_[Xexplore_sample, Xtest],\n",
    "                          np.r_[actions_explore_sample, actions_test_biased],\n",
    "                          np.r_[rewards_explore_sample, rewards_test_biased])\n",
    "np.random.seed(1)\n",
    "est_r_dr_whole = evaluateDoublyRobust(pred, X=Xtest, a=actions_test_biased, r=rewards_test_biased,\\\n",
    "                p=prob_actions_test_biased, reward_estimator = model_fit_on_all_data)\n",
    "\n",
    "# method 2: estimating rewards by fitting another model to the test data only\n",
    "np.random.seed(1)\n",
    "est_r_dr_test_only = evaluateDoublyRobust(pred, X=Xtest, a=actions_test_biased, r=rewards_test_biased,\\\n",
    "                p=prob_actions_test_biased, reward_estimator = LogisticRegression(), nchoices=y.shape[1])\n",
    "\n",
    "print('Biased Test set mean reward estimates (new policy)')\n",
    "print('DR estimate (reward estimator fit on train+test): ', est_r_dr_whole)\n",
    "print('DR estimate (reward estimator fit on test only): ', est_r_dr_test_only)\n",
    "print('----------------')\n",
    "print('Real mean reward: ', real_r)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.781681823550254, 16934)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Finally, rejection sampling can also be used to evaluate online policies - \n",
    "#in this case though, be aware that the estimate will only be considered up to\n",
    "#a certain number of rounds (as many as it accepts, but it will end up rejecting the majority),\n",
    "#but online policies keep improving with time.\n",
    "\n",
    "from contextualbandits.online import BootstrappedUCB\n",
    "\n",
    "\n",
    "features, y = parse_data(\"data.txt\")\n",
    "nchoices = y.shape[1]\n",
    "\n",
    "Xall=features\n",
    "actions_random = np.random.randint(nchoices, size = Xall.shape[0])\n",
    "rewards_actions = y[np.arange(y.shape[0]), actions_random]\n",
    "\n",
    "online_policy = BootstrappedUCB(LogisticRegression(max_iter=15000), y.shape[1])\n",
    "evaluateRejectionSampling(online_policy,\n",
    "                          X = Xall,\n",
    "                          a = actions_random,\n",
    "                          r = rewards_actions,\n",
    "                          online = True,\n",
    "                          start_point_online = 'random',\n",
    "                          batch_size = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
